# -*- coding: utf-8 -*-
"""Assingmnent 2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1COGhrIIvG4tBv05SevN6ShBH5vKeyTbC
"""

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

from google.colab import drive
    drive.mount('/content/drive')

from google.colab import files
files.upload()

!unzip -q dp_mlm_project.zip

# Commented out IPython magic to ensure Python compatibility.
# %cd dp_mlm_project

!pip install evaluate tf-keras

# This will privatize the GLUE SST-2 dataset with an epsilon of 50
!python scripts/privatize_glue.py --eps 50

!pip install --upgrade transformers datasets evaluate

!pip install -U "transformers>=4.44" "datasets>=2.20" "evaluate>=0.4" accelerate

!python -u scripts/train_task.py --eps 50.0

!python scripts/privatize_glue.py --eps 10

!python scripts/train_task.py --eps 10

!python scripts/privatize_glue.py --eps 25

!python scripts/train_task.py --eps 25

!python scripts/privatize_glue.py --eps 100

!python scripts/train_task.py --eps 100

!python scripts/privatize_glue.py --eps 250

!python scripts/train_task.py --eps 250

import pandas as pd, random

eps_values = [10.0, 25.0, 50.0, 100.0, 250.0]
samples = {}

# Load one example sentence per epsilon
for eps in eps_values:
    df = pd.read_csv(f"data/privatized_sst2/eps_{eps}/validation.csv")
    # make sure your privatized files have 'sentence' column
    samples[eps] = df.sample(1, random_state=42)["sentence"].values[0]

# Create a qualitative comparison table
print("Qualitative comparison of privatized sentences across ε:")
for eps, text in samples.items():
    print(f"ε = {eps}: {text}")

import pandas as pd, random, os
random.seed(42)

in_path = "data/privatized_sst2/eps_50.0/validation.csv"
assert os.path.exists(in_path), f"Missing file: {in_path}"

df = pd.read_csv(in_path)

# If your CSV has different column names, adjust here:
TEXT_COL = "sentence"
LABEL_COL = "label" if "label" in df.columns else None

# Sample a few qualitative examples
k = min(8, len(df))
sample = df.sample(k, random_state=42)

# Build a compact LaTeX table (wrap long text)
def escape_latex(s: str) -> str:
    return (s.replace("&","\\&").replace("%","\\%").replace("$","\\$")
              .replace("#","\\#").replace("_","\\_").replace("{","\\{")
              .replace("}","\\}").replace("~","\\textasciitilde{}")
              .replace("^","\\textasciicircum{}").replace("\\","\\textbackslash{}"))

rows = []
for _, r in sample.iterrows():
    sent = escape_latex(str(r[TEXT_COL])[:160])  # clip to avoid over-wide table
    lab = str(int(r[LABEL_COL])) if LABEL_COL else ""
    rows.append((sent, lab))

tex = [
r"\begin{table}[t]",
r"\centering",
r"\caption{Qualitative examples from the privatized SST-2 validation set at $\epsilon=50$.}",
r"\label{tab:qual-sst2-eps50}",
r"\begin{tabular}{p{0.82\linewidth} r}",
r"\toprule",
r"\textbf{Privatized sentence} & \textbf{Label} \\",
r"\midrule",
]
for sent, lab in rows:
    line = f"{sent} & {lab} \\\\"
    tex.append(line)
tex += [
r"\bottomrule",
r"\end{tabular}",
r"\end{table}",
]
os.makedirs("runs", exist_ok=True)
with open("runs/qual_eps50.tex","w") as f:
    f.write("\n".join(tex))

print("Wrote LaTeX table to runs/qual_eps50.tex")

import pandas as pd, numpy as np, os, string, collections

p = "data/privatized_sst2/eps_50.0/validation.csv"
df = pd.read_csv(p)

def tokenize(s): return str(s).split()

lengths = df["sentence"].apply(lambda s: len(tokenize(s)))
avg_len = float(lengths.mean())
std_len = float(lengths.std())

# crude vocab size
counter = collections.Counter()
for s in df["sentence"]:
    counter.update(tokenize(s))
vocab_size = len(counter)

print({"epsilon": 50.0, "avg_len": avg_len, "std_len": std_len, "vocab_size": vocab_size})

# Save a tiny LaTeX snippet
with open("runs/stats_eps50.tex","w") as f:
    f.write(r"""\begin{table}[t]
\centering
\caption{Structural statistics of privatized validation (SST-2) at $\epsilon=50$.}
\label{tab:stats-eps50}
\begin{tabular}{l r}
\toprule
Average length (tokens) & %.2f \\
Std. dev. length & %.2f \\
Vocabulary size & %d \\
\bottomrule
\end{tabular}
\end{table}
""" % (avg_len, std_len, vocab_size))
print("Wrote LaTeX table to runs/stats_eps50.tex")

# form_email_gen.py
import random, os, csv
random.seed(42)

N = 200
FIRST = ["Alex","Jordan","Taylor","Casey","Morgan","Sam","Avery","Jamie","Riley","Quinn"]
ORG   = ["Finance","Marketing","Research","Operations","Support","Legal","Security","HR","Design","Analytics"]
ACT   = ["submit the report","reschedule the meeting","review the draft","update the budget","confirm attendance",
         "share the slides","approve the request","finalize the agenda","prepare the summary","send the invoice"]
DATE  = ["Monday","Tuesday","Wednesday","Thursday","Friday","next week","this afternoon","tomorrow morning"]
LOC   = ["Sydney","Melbourne","Brisbane","Perth","Adelaide","Canberra","Hobart"]

os.makedirs("data/form_email", exist_ok=True)
with open("data/form_email/original.txt", "w") as ftxt, open("data/form_email/original.csv", "w", newline="") as fcsv:
    w = csv.writer(fcsv)
    w.writerow(["id","sentence"])
    for i in range(N):
        s = (f"Hi {random.choice(FIRST)}, please {random.choice(ACT)} "
             f"by {random.choice(DATE)} for the {random.choice(ORG)} team in {random.choice(LOC)}.")
        ftxt.write(s + "\n")
        w.writerow([i, s])

print("Wrote data/form_email/original.{txt,csv}")

!python form_email_gen.py

for eps in [10, 25, 50, 100, 250]:
    !python scripts/rewrite.py \
        --model roberta-base \
        --clip 10 \
        --eps {eps} \
        --input data/form_email/original.txt \
        --out runs/form_email/

!pip install sacrebleu

!python /content/dp_mlm_project/eval_form_email.py

!find runs -type f | grep eps_

import pandas as pd, matplotlib.pyplot as plt, os

os.makedirs("runs/form_email", exist_ok=True)
df = pd.read_csv("runs/form_email/eval_results.csv").sort_values("epsilon")

# ---- LaTeX table ----
tex = r"""\begin{table}[t]
\centering
\caption{Form-Email results across privacy budgets (BLEU $\uparrow$, change rate $\downarrow$).}
\label{tab:form-email}
\begin{tabular}{r r r r}
\toprule
$\epsilon$ & BLEU (\%) $\uparrow$ & Change rate $\downarrow$ & Avg.\ length \\
\midrule
"""
for _, r in df.iterrows():
    tex += f"{int(r.epsilon)} & {r.BLEU:.2f} & {r.change_rate:.2f} & {r.avg_len:.2f} \\\\\n"
tex += r"""\bottomrule
\end{tabular}
\end{table}
"""
with open("runs/form_email/table_form_email.tex","w") as f:
    f.write(tex)
print("Wrote runs/form_email/table_form_email.tex")

# ---- Plot BLEU + change rate ----
fig, ax1 = plt.subplots()
ax1.plot(df["epsilon"], df["BLEU"], marker="o")
ax1.set_xlabel("Privacy budget (ε)")
ax1.set_ylabel("BLEU (higher = more similar)")
ax1.grid(True)

ax2 = ax1.twinx()
ax2.plot(df["epsilon"], df["change_rate"], marker="s")
ax2.set_ylabel("Change rate (lower = closer to original)")
plt.title("Form-Email: Utility vs Privacy")
plt.tight_layout()
plt.savefig("runs/form_email/form_email_metrics.png", dpi=200)
print("Wrote runs/form_email/form_email_metrics.png")

import pandas as pd
with open("data/form_email/original.txt") as f:
    orig = [l.strip() for l in f if l.strip()]

def head(path, k=3):
    with open(path) as f:
        priv = [l.strip() for l in f if l.strip()]
    for i in range(k):
        print(f"- ORIG: {orig[i]}")
        print(f"  EPS : {priv[i]}\n")

print("ε=10 examples:")
head("runs/form_email/rewritten_eps_10.0.txt", 3)
print("\nε=100 examples:")
head("runs/form_email/rewritten_eps_100.0.txt", 3)